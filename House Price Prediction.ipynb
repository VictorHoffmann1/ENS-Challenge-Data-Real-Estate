{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c8576d",
   "metadata": {},
   "source": [
    "# House Price Prediction with Images and Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5125e0",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e833b",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_test= pd.read_csv('./Challenge/Data/X_test.csv')\n",
    "X_train_raw = pd.read_csv('./Challenge/Data/X_train.csv')\n",
    "y_random_raw = pd.read_csv('./Challenge/Data/y_random.csv')\n",
    "y_train_raw = pd.read_csv('./Challenge/Data/y_train.csv')\n",
    "train = pd.merge(X_train_raw, y_train_raw, how = \"outer\", on = \"id_annonce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a1e81",
   "metadata": {},
   "source": [
    "### Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26183457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import skimage.measure \n",
    "path_train = './Challenge/Data/reduced_images_ILB/reduced_images/train'\n",
    "path_test = './Challenge/Data/reduced_images_ILB/reduced_images/test'\n",
    "test_annonce = np.array(X_test.id_annonce)\n",
    "train_annonce = np.array(train.id_annonce)\n",
    "\n",
    "#TRAIN IMAGES\n",
    "\n",
    "dict = {'id_annonce':[]}\n",
    "dict['Entropy'] = [] # Calculating the image's entropy and inserting it in the table.\n",
    "image_features = []\n",
    "for annonce in train_annonce:\n",
    "    path = path_train+f'/ann_{annonce}'\n",
    "    images_path = os.listdir(path)\n",
    "    entropy = []\n",
    "    output_image = np.zeros((140,210), dtype = np.int8)\n",
    "    count = 0\n",
    "    for i in images_path:\n",
    "        m = count//3\n",
    "        n = count%3\n",
    "        img0 = Image.open(path+f'/{i}')\n",
    "        img = img0.convert('L')\n",
    "        imagea = np.asarray(img, dtype='uint8')\n",
    "        image = cv2.resize(imagea, (70,70)).tolist()\n",
    "        entropy.append(skimage.measure.shannon_entropy(image))\n",
    "         # As each house has between 1 and 6 images, I put each images in a 210*140 image \n",
    "        output_image[m*70:(m+1)*70,n*70:(n+1)*70] = image\n",
    "        count += 1\n",
    "    image_features.append(output_image)\n",
    "    dict['Entropy'].append(np.mean(entropy))\n",
    "    dict['id_annonce'].append(annonce)\n",
    "\n",
    "dict['Image'] = image_features\n",
    "train_images = pd.DataFrame(data=dict)\n",
    "\n",
    "#TEST IMAGES\n",
    "\n",
    "dict = {'id_annonce':[]}\n",
    "dict['Entropy'] = []\n",
    "image_features =[]\n",
    "for annonce in test_annonce:\n",
    "    path = path_test+f'/ann_{annonce}'\n",
    "    images_path = os.listdir(path)\n",
    "    entropy = []\n",
    "    output_image = np.zeros((140,210), dtype = np.int8)\n",
    "    count = 0\n",
    "    for i in images_path:\n",
    "        m = count//3\n",
    "        n = count%3\n",
    "        img0 = Image.open(path+f'/{i}')\n",
    "        img = img0.convert('L')\n",
    "        imagea = np.asarray(img, dtype='uint8')\n",
    "        image = cv2.resize(imagea, (70,70)).tolist()\n",
    "        entropy.append(skimage.measure.shannon_entropy(image))\n",
    "        output_image[m*70:(m+1)*70,n*70:(n+1)*70] = image\n",
    "        count += 1\n",
    "    image_features.append(output_image)\n",
    "    dict['Entropy'].append(np.mean(entropy))\n",
    "    dict['id_annonce'].append(annonce)\n",
    "\n",
    "dict['Image'] = image_features\n",
    "test_images = pd.DataFrame(data=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b75e0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_str = np.array(['property_type','energy_performance_category','ghg_category','exposition','city'])\n",
    "\n",
    "\n",
    "# DROPPING USELESS VARIABLES (FEATURE SELECTION THANKS TO A XGBOOST MODEL)\n",
    "train = train.drop(['last_floor','upper_floors'],axis=1)\n",
    "X_test_raw = X_test.copy()\n",
    "X_test = X_test.drop(['last_floor','upper_floors'],axis=1)\n",
    "\n",
    "#TARGET ENCODING CATEGORICAL VALUES\n",
    "from category_encoders import TargetEncoder\n",
    "encoder = TargetEncoder(handle_missing = 'return_nan')\n",
    "\n",
    "for i in columns_str:\n",
    "    train[[f'{i}']] = encoder.fit_transform(train[f'{i}'], train['price'], )\n",
    "    X_test[[f'{i}']] = encoder.transform(X_test[f'{i}'])\n",
    "    \n",
    "y_train = train.pop('price')\n",
    "\n",
    "columns_train = np.array(train.columns)\n",
    "\n",
    "\n",
    "#NORMALIZING THE DATA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(train),columns = train.columns)\n",
    "scaler = RobustScaler()\n",
    "X_test = pd.DataFrame(scaler.fit_transform(X_test),columns = X_test.columns)\n",
    "\n",
    "# KNN IMPUTING NAN VARIABLES\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=21)\n",
    "KNN_train = pd.DataFrame(imputer.fit_transform(train),columns = train.columns)\n",
    "imputer = KNNImputer(n_neighbors=21)\n",
    "KNN_test = pd.DataFrame(imputer.fit_transform(X_test),columns = X_test.columns)\n",
    "\n",
    "\n",
    "# Function to add importance vairable and KNN-Impute\n",
    "def impute_nan_add_vairable(DataFrame,ColName, KNN_DF):\n",
    "    #1. add new column and replace if category is NaN then 1 else 0\n",
    "    DataFrame[ColName+\"_Imputed\"] =   np.where(DataFrame[ColName].isna(),1,0)\n",
    "    \n",
    "    # 2. Take most occured category in that vairable (.mode())\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=21)\n",
    "    \n",
    "    ## 2.1 Replace NAN values with most occured category in actual vairable\n",
    "    \n",
    "    DataFrame[ColName] = KNN_DF[ColName]\n",
    "# Call function to impute NAN values and add new importance feature\n",
    "\n",
    "\n",
    "\n",
    "for i in columns_train:\n",
    "    if i in columns_str:\n",
    "        impute_nan_add_vairable(train,i,KNN_train)\n",
    "    else:\n",
    "        train[i] = KNN_train[i]\n",
    "        \n",
    "columns_X_test = np.array(X_test.columns)\n",
    "for i in columns_X_test:\n",
    "    if i in columns_str:\n",
    "        impute_nan_add_vairable(X_test,i,KNN_test)\n",
    "    else:\n",
    "        median = np.nanmedian(np.array(X_test[[i]]).flatten())\n",
    "        X_test[i] = KNN_test[i]\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e319a9",
   "metadata": {},
   "source": [
    "## Creating a train and a test ensemble for tabular data and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58657e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Complete_train = pd.merge(train, train_images, how = \"outer\", on = \"id_annonce\")\n",
    "Complete_test = pd.merge(X_test, test_images, how = \"outer\", on = \"id_annonce\")\n",
    "\n",
    "\n",
    "image_data = Complete_train.pop('Image')\n",
    "image_data_test = Complete_test.pop('Image')\n",
    "Complete_train.pop('id_annonce')\n",
    "Complete_test.pop('id_annonce')\n",
    "y_train = Complete_train.pop('price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.array(image_data)\n",
    "image_data = image_data.tolist()\n",
    "image_data = np.asarray(image_data, dtype=np.int8)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_test = np.array(image_data_test)\n",
    "image_data_test = image_data_test.tolist()\n",
    "image_data_test = np.asarray(image_data_test)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8f949",
   "metadata": {},
   "source": [
    "# Concatenation of a CNN and a MLP with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfb3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing tabular data.\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(Complete_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33becedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential(normalizer)\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "    # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "# create the MLP and CNN models\n",
    "mlp = create_mlp(regress=False)\n",
    "cnn = create_cnn(70,70, 1, regress=False)\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(1e-3)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "    x=[Complete_train, image_data], y=y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634ed37",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786137b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [price]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([Complete_test, image_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['price']] = y_pred.reshape(len(y_pred),1)\n",
    "submission = X_test[['id_annonce','price']]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07409623",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Submission.csv',header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
