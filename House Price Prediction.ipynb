{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c8576d",
   "metadata": {},
   "source": [
    "# House Price Prediction with Images and Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5125e0",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e833b",
   "metadata": {},
   "source": [
    "### Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087e815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_test= pd.read_csv('./Challenge/Data/X_test.csv')\n",
    "X_train_raw = pd.read_csv('./Challenge/Data/X_train.csv')\n",
    "y_random_raw = pd.read_csv('./Challenge/Data/y_random.csv')\n",
    "y_train_raw = pd.read_csv('./Challenge/Data/y_train.csv')\n",
    "train = pd.merge(X_train_raw, y_train_raw, how = \"outer\", on = \"id_annonce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a1e81",
   "metadata": {},
   "source": [
    "### Image Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26183457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import skimage.measure \n",
    "path_train = './Challenge/Data/reduced_images_ILB/reduced_images/train'\n",
    "path_test = './Challenge/Data/reduced_images_ILB/reduced_images/test'\n",
    "test_annonce = np.array(X_test.id_annonce)\n",
    "train_annonce = np.array(train.id_annonce)\n",
    "\n",
    "#TRAIN IMAGES\n",
    "\n",
    "dict = {'id_annonce':[]}\n",
    "dict['Entropy'] = [] # Calculating the image's entropy and inserting it in the table.\n",
    "image_features = []\n",
    "for annonce in train_annonce:\n",
    "    path = path_train+f'/ann_{annonce}'\n",
    "    images_path = os.listdir(path)\n",
    "    entropy = []\n",
    "    output_image = np.zeros((140,210), dtype = np.int8)\n",
    "    count = 0\n",
    "    for i in images_path:\n",
    "        m = count//3\n",
    "        n = count%3\n",
    "        img0 = Image.open(path+f'/{i}')\n",
    "        img = img0.convert('L')\n",
    "        imagea = np.asarray(img, dtype='uint8')\n",
    "        image = cv2.resize(imagea, (70,70)).tolist()\n",
    "        entropy.append(skimage.measure.shannon_entropy(image))\n",
    "         # As each house has between 1 and 6 images, I put each images in a 210*140 image \n",
    "        output_image[m*70:(m+1)*70,n*70:(n+1)*70] = image\n",
    "        count += 1\n",
    "    image_features.append(output_image)\n",
    "    dict['Entropy'].append(np.mean(entropy))\n",
    "    dict['id_annonce'].append(annonce)\n",
    "\n",
    "dict['Image'] = image_features\n",
    "train_images = pd.DataFrame(data=dict)\n",
    "\n",
    "#TEST IMAGES\n",
    "\n",
    "dict = {'id_annonce':[]}\n",
    "dict['Entropy'] = []\n",
    "image_features =[]\n",
    "for annonce in test_annonce:\n",
    "    path = path_test+f'/ann_{annonce}'\n",
    "    images_path = os.listdir(path)\n",
    "    entropy = []\n",
    "    output_image = np.zeros((140,210), dtype = np.int8)\n",
    "    count = 0\n",
    "    for i in images_path:\n",
    "        m = count//3\n",
    "        n = count%3\n",
    "        img0 = Image.open(path+f'/{i}')\n",
    "        img = img0.convert('L')\n",
    "        imagea = np.asarray(img, dtype='uint8')\n",
    "        image = cv2.resize(imagea, (70,70)).tolist()\n",
    "        entropy.append(skimage.measure.shannon_entropy(image))\n",
    "        output_image[m*70:(m+1)*70,n*70:(n+1)*70] = image\n",
    "        count += 1\n",
    "    image_features.append(output_image)\n",
    "    dict['Entropy'].append(np.mean(entropy))\n",
    "    dict['id_annonce'].append(annonce)\n",
    "\n",
    "dict['Image'] = image_features\n",
    "test_images = pd.DataFrame(data=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9b75e0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_str = np.array(['property_type','energy_performance_category','ghg_category','exposition'])\n",
    "columns_target = np.array(['property_type','exposition'])\n",
    "columns_ghg = np.array(['energy_performance_category','ghg_category'])\n",
    "\n",
    "y_train = np.log1p(train.pop('price'))\n",
    "\n",
    "features = pd.concat([train, X_test]).reset_index(drop=True)\n",
    "\n",
    "# DROPPING USELESS VARIABLES (FEATURE SELECTION THANKS TO A XGBOOST MODEL)\n",
    "features = features.drop(['id_annonce','upper_floors','city','postal_code'],axis=1)\n",
    "\n",
    "\n",
    "features[['land_size','floor']] = features[['land_size','floor']].fillna(0)\n",
    "\n",
    "#ADDING OTHER VARIABLES\n",
    "features['TotalSize'] = features['size']+features['land_size']\n",
    "\n",
    "ghg_dict = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'None':4}\n",
    "for column in columns_str:\n",
    "    features[column+\"_Imputed\"] = np.where(features[column].isna(),1,0)\n",
    "    features[column] = features[column].fillna('None')\n",
    "for column in columns_ghg:\n",
    "    features[column] = features[column].replace(ghg_dict)\n",
    "\n",
    "columns = np.array(features.columns)    \n",
    "\n",
    "for i in columns:\n",
    "    if i not in columns_str:\n",
    "        median = np.nanmedian(np.array(features[[i]]).flatten())\n",
    "        features[i] = features[i].fillna(median)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "columns_num = []\n",
    "for i in features.columns:\n",
    "    if features[i].dtype in numeric_dtypes:\n",
    "        columns_num.append(i)\n",
    "\n",
    "columns_bool = ['nb_parking_places','nb_boxes','has_a_balcony','nb_terraces','has_a_cellar','has_a_garage','has_air_conditioning','last_floor']\n",
    "\n",
    "columns_float = []\n",
    "for i in columns_num:\n",
    "    if i not in columns_bool:\n",
    "        columns_float.append(i)\n",
    "from scipy.stats import skew\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "skew_features = features[columns_float].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "\n",
    "high_skew = skew_features[skew_features > 0.5]\n",
    "skew_index = high_skew.index\n",
    "\n",
    "for i in skew_index:\n",
    "    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26203e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = pd.get_dummies(features).reset_index(drop=True)\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = final_features.iloc[:len(y_train), :]\n",
    "X_test = final_features.iloc[len(y_train):, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e319a9",
   "metadata": {},
   "source": [
    "## Creating a train and a test ensemble for tabular data and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58657e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Complete_train = pd.merge(train, train_images, how = \"outer\", on = \"id_annonce\")\n",
    "Complete_test = pd.merge(X_test, test_images, how = \"outer\", on = \"id_annonce\")\n",
    "\n",
    "\n",
    "image_data = Complete_train.pop('Image')\n",
    "image_data_test = Complete_test.pop('Image')\n",
    "Complete_train.pop('id_annonce')\n",
    "Complete_test.pop('id_annonce')\n",
    "y_train = Complete_train.pop('price')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a87b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.array(image_data)\n",
    "image_data = image_data.tolist()\n",
    "image_data = np.asarray(image_data, dtype=np.int8)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_test = np.array(image_data_test)\n",
    "image_data_test = image_data_test.tolist()\n",
    "image_data_test = np.asarray(image_data_test)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c8f949",
   "metadata": {},
   "source": [
    "# Concatenation of a CNN and a MLP with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfb3a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c0a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing tabular data.\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(np.array(Complete_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33becedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(regress=False):\n",
    "    # define our MLP network\n",
    "    model = Sequential(normalizer)\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(32, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    # return our model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
    "    # initialize the input shape and channel dimension, assuming\n",
    "    # TensorFlow/channels-last ordering\n",
    "    inputShape = (height, width, depth)\n",
    "    chanDim = -1\n",
    "    # define the model input\n",
    "    inputs = Input(shape=inputShape)\n",
    "    # loop over the number of filters\n",
    "    for (i, f) in enumerate(filters):\n",
    "        # if this is the first CONV layer then set the input\n",
    "    # appropriately\n",
    "        if i == 0:\n",
    "            x = inputs\n",
    "        # CONV => RELU => BN => POOL\n",
    "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=chanDim)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    # flatten the volume, then FC => RELU => BN => DROPOUT\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    # apply another FC layer, this one to match the number of nodes\n",
    "    # coming out of the MLP\n",
    "    x = Dense(4)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    # check to see if the regression node should be added\n",
    "    if regress:\n",
    "        x = Dense(1, activation=\"linear\")(x)\n",
    "    # construct the CNN\n",
    "    model = Model(inputs, x)\n",
    "    # return the CNN\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49bc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate\n",
    "# create the MLP and CNN models\n",
    "mlp = create_mlp(regress=False)\n",
    "cnn = create_cnn(70,70, 1, regress=False)\n",
    "# create the input to our final set of layers as the *output* of both\n",
    "# the MLP and CNN\n",
    "combinedInput = concatenate([mlp.output, cnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"linear\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b6791",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(1e-3)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "model.fit(\n",
    "    x=[Complete_train, image_data], y=y_train,\n",
    "    validation_split = 0.2,\n",
    "    epochs=50, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634ed37",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786137b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [price]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(model.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([Complete_test, image_data_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11cf3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[['price']] = y_pred.reshape(len(y_pred),1)\n",
    "submission = X_test[['id_annonce','price']]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07409623",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Submission.csv',header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
